{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# train\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# load data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,)),  # mean value = 0.1307, standard deviation value = 0.3081\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './MNIST'\n",
    "\n",
    "training_set = datasets.MNIST(root = data_path, train= True, download=True, transform= transform)\n",
    "testing_set = datasets.MNIST(root = data_path, train= False, download=True, transform= transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* design a neural network that consists of three `fully connected layers` with an activation function of `Sigmoid`\n",
    "* the activation function for the output layer is `LogSoftmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classification, self).__init__()\n",
    "        \n",
    "        # construct layers for a neural network\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=20*20),\n",
    "            nn.Sigmoid(),\n",
    "        ) \n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(in_features=20*20, out_features=10*10),\n",
    "            nn.Sigmoid(),\n",
    "        ) \n",
    "        self.classifier3 = nn.Sequential(\n",
    "            nn.Linear(in_features=10*10, out_features=10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        ) \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):                 # [batchSize, 1, 28, 28]\n",
    "        x = inputs.view(inputs.size(0), -1)    # [batchSize, 28*28]\n",
    "        x = self.classifier1(x)                # [batchSize, 20*20]\n",
    "        x = self.classifier2(x)                # [batchSize, 10*10]\n",
    "        out = self.classifier3(x)              # [batchSize, 10]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "* use a stochastic gradient descent algorithm with different mini-batch sizes of 32, 64, 128\n",
    "* use a constant learning rate for all the mini-batch sizes\n",
    "* do not use any regularization algorithm such as `dropout` or `weight decay`\n",
    "* compute the average loss and the average accuracy for all the mini-batches within each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc) * 100\n",
    "    \n",
    "    return acc\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "#     \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "#     \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: | Train Loss: 0.36082 | Train Acc: 96.373\n",
      "Epoch 1: | Train Loss: 0.11445 | Train Acc: 100.000\n",
      "Epoch 2: | Train Loss: 0.07273 | Train Acc: 100.000\n",
      "Epoch 3: | Train Loss: 0.04961 | Train Acc: 100.000\n",
      "Epoch 4: | Train Loss: 0.03417 | Train Acc: 100.000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lr=0.5\n",
    "n_epochs = 5\n",
    "\n",
    "no_cuda = True\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=testing_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "classifier = classification().to(device)\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    classifier.train()\n",
    "    \n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = classifier(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "#     # VALIDATION    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         val_epoch_loss = 0\n",
    "#         val_epoch_acc = 0\n",
    "        \n",
    "#         classifier.eval()\n",
    "#         for X_val_batch, y_val_batch in test_loader:\n",
    "#             X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "#             y_val_pred = classifier(X_val_batch)\n",
    "                        \n",
    "#             val_loss = criterion(y_val_pred, y_val_batch)\n",
    "#             val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "#             val_epoch_loss += val_loss.item()\n",
    "#             val_epoch_acc += val_acc.item()\n",
    "            \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "#     loss_stats['val'].append(val_epoch_loss/len(test_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "#     accuracy_stats['val'].append(val_epoch_acc/len(test_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {epoch}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plot the training and testing losses with a batch size of 32 [4pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot the training and testing accuracies with a batch size of 32 [4pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot the training and testing losses with a batch size of 64 [4pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot the training and testing accuracies with a batch size of 64 [4pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the training and testing losses with a batch size of 128 [4pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot the training and testing accuracies with a batch size of 128 [4pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Print the loss at convergence with different mini-batch sizes [3pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Print the accuracy at convergence with different mini-batch sizes [3pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
